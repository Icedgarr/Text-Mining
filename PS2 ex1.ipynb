{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement the uncollapsed Gibbs sampler for latent Dirichlet allocation we dis- cussed in class. Apply it to state-of-the-union addresses at a level of aggregation you choose and describe the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the environment and take care of all the boring stuff (we will recycle code from the previous exercise to handle the text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_table(path+\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data = data.loc[data['year']>=1946]\n",
    "data = data.reset_index()\n",
    "\n",
    "def data_preparation(data):\n",
    "    prep_data = data.apply(lambda row: #tokenize\n",
    "                            word_tokenize(row['speech'].lower()), axis=1)\n",
    "    stop_w=set(stopwords.words('english')) #stopwords\n",
    "    for i in range(len(prep_data)): #non-alphanumeric characters\n",
    "        prep_data[i] = [w for w in prep_data[i] if w not in stop_w and w.isalpha()]\n",
    "    stemmer = PorterStemmer() #Create a stemmer object\n",
    "    for i in range(len(prep_data)): #Stem the data\n",
    "        prep_data[i] = [stemmer.stem(elem) for elem in prep_data[i]]\n",
    "    unique_words = np.unique([word for doc in prep_data for word in doc]).tolist() #List of unique words\n",
    "    D = len(prep_data)\n",
    "    V = len(unique_words)\n",
    "    X = np.zeros((D,V)) #The document-term matrix\n",
    "    N = 0\n",
    "    for i in range(D):\n",
    "        N = N + len(prep_data[i])\n",
    "        aux_words_d = list(set(prep_data[i]))\n",
    "        for j in range(len(aux_words_d)):\n",
    "            X[i,unique_words.index(aux_words_d[j])] = prep_data[i].count(aux_words_d[j])\n",
    "    X = csr_matrix(X.astype(int))\n",
    "    return prep_data, unique_words, X, N\n",
    "\n",
    "prep_data, unique_words, X, N = data_preparation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample topic allocation\n",
    "First, we create a function that simulates from a multinomial and actually returns draws from a multinomial. That way, we can generate an initial guess for the Z 'matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(K, row):\n",
    "    samples = np.random.multinomial(1,[1/K]*K,len(prep_data[row])).tolist()\n",
    "    samples_correct = []\n",
    "    for s in samples:\n",
    "        samples_correct.append(s.index(1))\n",
    "    return samples_correct\n",
    "\n",
    "Z = prep_data.apply(lambda row: simulate(K,row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Z is not a matrix, but a list of sublists. There are D sublists (one for each document), each one containing n_d entries (different documents have different number of words).\n",
    "Now, we create a function that samples from those topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_topic(Z, theta, beta):\n",
    "    D = len(Z)\n",
    "    for d in range(D):\n",
    "        n = len(Z[d])\n",
    "        for i in range(n):\n",
    "            beta_v = beta[unique_words.index(prep_data[d][i])]\n",
    "            probs = (theta[d,:]*beta_v)/np.sum(theta[d,:]*beta_v)\n",
    "            Z[d][i] = np.random.multinomial(1, probs).tolist().index(1)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample theta\n",
    "\n",
    "For the theta, we'll need two functions: (i) a function that generates the number of counts per document and topic, and (ii) the function that actually samples from theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_count(Z_d, K):\n",
    "    N_count_vector = []\n",
    "    for k in range(K):\n",
    "        N_count_vector.append(Z_d.count(k))\n",
    "    return N_count_vector\n",
    "\n",
    "def sample_theta(Z,alpha,theta):\n",
    "    D,K = theta.shape\n",
    "    N = np.zeros((D,K))\n",
    "    for d in range(D):\n",
    "        N[d,:] = N_count(Z[d], K)\n",
    "        theta[d,:] = dirichlet(N[d,:] + alpha)\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample beta\n",
    "\n",
    "Now, we will create a function that generates the betas. Note that it includes the script to generate the M, which it is needed to generate the betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_beta(Z,prep_data,eta,beta):\n",
    "    K = beta.shape[1]\n",
    "    M = np.zeros((K,V))\n",
    "    #Generate M\n",
    "    s = [i for sublist in prep_data for i in sublist ]\n",
    "    z_s = [z for sublist in Z for z in sublist]\n",
    "    for k in range(K):\n",
    "        words = [s[i] for i in range(len(s)) if z_s[i] == k]\n",
    "        counts = Counter(words)\n",
    "        for v in range(len(unique_words)):\n",
    "            if unique_words[v] in counts: M[k,v] = counts[unique_words[v]]\n",
    "    #Generate beta\n",
    "    for k in range(K):\n",
    "        beta[:,k] = dirichlet(M[k,:] + eta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampler\n",
    "\n",
    "Finally, we put it all together inside a function, iterate and compute the perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sampler(n_iter,prep_data,alpha,eta, K, X, N, prop_perplexity):\n",
    "    ## Initialize objects\n",
    "    D = len(prep_data)\n",
    "    theta = dirichlet([alpha]*K,D)\n",
    "    beta = dirichlet([eta]*K,V)\n",
    "    Z = prep_data.apply(lambda row: simulate(K,row))\n",
    "    Z_dist = []\n",
    "    theta_dist = []\n",
    "    beta_dist = []\n",
    "    perplexity = []\n",
    "    for i in range(n_iter):\n",
    "        print('Iteration nº:'+ str(i))\n",
    "        start = time.time()\n",
    "        Z = sample_topic(Z,theta,beta)\n",
    "        theta = sample_theta(Z,alpha,theta)\n",
    "        beta = sample_beta(Z,prep_data,eta,beta)\n",
    "        if (i % (round(n_iter * prop_perplexity) + 1)) == 0:\n",
    "            perplexity.append(np.exp(-np.sum(X.multiply(np.log(theta.dot(beta.T))))/N))\n",
    "            np.save(path2+\"perplexity.npy\",perplexity)\n",
    "        Z_dist.append(Z)\n",
    "        theta_dist.append(theta)\n",
    "        beta_dist.append(beta)\n",
    "        np.save(path2+\"theta.npy\",theta_dist)\n",
    "        np.save(path2+\"Z_dist.npy\",Z_dist)\n",
    "        np.save(path2+\"beta_dist.npy\",beta_dist)\n",
    "        print('Duration:'+ str(time.time()-start))\n",
    "    return Z_dist, beta_dist, theta_dist, perplexity\n",
    "\n",
    "\n",
    "#Initial values (reference original paper)\n",
    "K = 2 #Number of topics\n",
    "alpha = 50/K\n",
    "V = len(unique_words)\n",
    "eta = 200/V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've tried with the parameters indicated above. Unfortunately, each iteration took 30 seconds, so we could not run a proper round. Trying with different parameters, we saw that for higher K, perplexity is lower. But it runs, so you can check it works (maybe try low number of iterations, it will show the progress in any case)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
