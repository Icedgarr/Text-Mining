{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement the uncollapsed Gibbs sampler for latent Dirichlet allocation we dis- cussed in class. Apply it to state-of-the-union addresses at a level of aggregation you choose and describe the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set up the environment and take care of all the boring stuff (we will recycle code from the previous exercise to handle the text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer, word_tokenize\n",
    "from numpy.random import dirichlet\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "data = pd.read_table(path+\"speech_data_extend.txt\",encoding=\"utf-8\")\n",
    "data = data.loc[data['year']>=1946]\n",
    "data = data.reset_index()\n",
    "\n",
    "def data_preparation(data):\n",
    "    prep_data = data.apply(lambda row: #tokenize\n",
    "                            word_tokenize(row['speech'].lower()), axis=1)\n",
    "    stop_w=set(stopwords.words('english')) #stopwords\n",
    "    for i in range(len(prep_data)): #non-alphanumeric characters\n",
    "        prep_data[i] = [w for w in prep_data[i] if w not in stop_w and w.isalpha()]\n",
    "    stemmer = PorterStemmer() #Create a stemmer object\n",
    "    for i in range(len(prep_data)): #Stem the data\n",
    "        prep_data[i] = [stemmer.stem(elem) for elem in prep_data[i]]\n",
    "    unique_words = np.unique([word for doc in prep_data for word in doc]).tolist() #List of unique words\n",
    "    D = len(prep_data)\n",
    "    V = len(unique_words)\n",
    "    X = np.zeros((D,V)) #The document-term matrix\n",
    "    N = 0\n",
    "    for i in range(D):\n",
    "        N = N + len(prep_data[i])\n",
    "        aux_words_d = list(set(prep_data[i]))\n",
    "        for j in range(len(aux_words_d)):\n",
    "            X[i,unique_words.index(aux_words_d[j])] = prep_data[i].count(aux_words_d[j])\n",
    "    X = csr_matrix(X.astype(int))\n",
    "    return prep_data, unique_words, X, N\n",
    "\n",
    "prep_data, unique_words, X, N = data_preparation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample topic allocation\n",
    "First, we create a function that simulates from a multinomial and actually returns draws from a multinomial. That way, we can generate an initial guess for the Z 'matrix'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate(K, row):\n",
    "    samples = np.random.multinomial(1,[1/K]*K,len(prep_data[row])).tolist()\n",
    "    samples_correct = []\n",
    "    for s in samples:\n",
    "        samples_correct.append(s.index(1))\n",
    "    return samples_correct\n",
    "\n",
    "Z = prep_data.apply(lambda row: simulate(K,row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Z is not a matrix, but a list of sublists. There are D sublists (one for each document), each one containing n_d entries (different documents have different number of words).\n",
    "Now, we create a function that samples from those topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_topic(Z, theta, beta):\n",
    "    D = len(Z)\n",
    "    for d in range(D):\n",
    "        n = len(Z[d])\n",
    "        for i in range(n):\n",
    "            beta_v = beta[unique_words.index(prep_data[d][i])]\n",
    "            probs = (theta[d,:]*beta_v)/np.sum(theta[d,:]*beta_v)\n",
    "            Z[d][i] = np.random.multinomial(1, probs).tolist().index(1)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample theta\n",
    "\n",
    "For the theta, we'll need two functions: (i) a function that generates the number of counts per document and topic, and (ii) the function that actually samples from theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_count(Z_d, K):\n",
    "    N_count_vector = []\n",
    "    for k in range(K):\n",
    "        N_count_vector.append(Z_d.count(k))\n",
    "    return N_count_vector\n",
    "\n",
    "def sample_theta(Z,alpha,theta):\n",
    "    D,K = theta.shape\n",
    "    N = np.zeros((D,K))\n",
    "    for d in range(D):\n",
    "        N[d,:] = N_count(Z[d], K)\n",
    "        theta[d,:] = dirichlet(N[d,:] + alpha)\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample beta\n",
    "\n",
    "Now, we will create a function that generates the betas. Note that it includes the script to generate the M, which it is needed to generate the betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_beta(Z,prep_data,eta,beta):\n",
    "    K = beta.shape[1]\n",
    "    M = np.zeros((K,V))\n",
    "    #Generate M\n",
    "    s = [i for sublist in prep_data for i in sublist ]\n",
    "    z_s = [z for sublist in Z for z in sublist]\n",
    "    for k in range(K):\n",
    "        words = [s[i] for i in range(len(s)) if z_s[i] == k]\n",
    "        counts = Counter(words)\n",
    "        for v in range(len(unique_words)):\n",
    "            if unique_words[v] in counts: M[k,v] = counts[unique_words[v]]\n",
    "    #Generate beta\n",
    "    for k in range(K):\n",
    "        beta[:,k] = dirichlet(M[k,:] + eta)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The sampler\n",
    "\n",
    "Finally, we put it all together inside a function, iterate and compute the perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gibbs_sampler(n_iter,prep_data,alpha,eta, K, X, N, prop_perplexity):\n",
    "    ## Initialize objects\n",
    "    D = len(prep_data)\n",
    "    theta = dirichlet([alpha]*K,D)\n",
    "    beta = dirichlet([eta]*K,V)\n",
    "    Z = prep_data.apply(lambda row: simulate(K,row))\n",
    "    Z_dist = []\n",
    "    theta_dist = []\n",
    "    beta_dist = []\n",
    "    perplexity = []\n",
    "    for i in range(n_iter):\n",
    "        print('Iteration nº:'+ str(i))\n",
    "        start = time.time()\n",
    "        Z = sample_topic(Z,theta,beta)\n",
    "        theta = sample_theta(Z,alpha,theta)\n",
    "        beta = sample_beta(Z,prep_data,eta,beta)\n",
    "        if (i % (round(n_iter * prop_perplexity) + 1)) == 0:\n",
    "            perplexity.append(np.exp(-np.sum(X.multiply(np.log(theta.dot(beta.T))))/N))\n",
    "            np.save(path2+\"perplexity.npy\",perplexity)\n",
    "        Z_dist.append(Z)\n",
    "        theta_dist.append(theta)\n",
    "        beta_dist.append(beta)\n",
    "        np.save(path2+\"theta.npy\",theta_dist)\n",
    "        np.save(path2+\"Z_dist.npy\",Z_dist)\n",
    "        np.save(path2+\"beta_dist.npy\",beta_dist)\n",
    "        print('Duration:'+ str(time.time()-start))\n",
    "    return Z_dist, beta_dist, theta_dist, perplexity\n",
    "\n",
    "\n",
    "#Initial values (reference original paper)\n",
    "K = 2 #Number of topics\n",
    "alpha = 50/K\n",
    "V = len(unique_words)\n",
    "eta = 200/V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've tried with the parameters indicated above. Unfortunately, each iteration took 30 seconds, so we could not run a proper round. Trying with different parameters, we saw that for higher K, perplexity is lower. But it runs, so you can check it works (maybe try low number of iterations, it will show the progress in any case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Now focus on individual paragraphs of state-of-the-union addresses from 1946 on- wards in which every paragraph is associated with one of two political parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's set up the environment and load everything that is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ecd6e393ad2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Select only documents which appear after 1945\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create a new variable with only presidents and years\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'president'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_words' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Select only documents which appear after 1945\n",
    "X = count_words(prep_data)\n",
    "# Create a new variable with only presidents and years\n",
    "parties = data.loc[:,['president', 'year']]\n",
    "parties = parties.reset_index()\n",
    "# Create a new variable with 1 when presidents are Democrats, 0 when they are Republicans\n",
    "zero_len = pd.Series(np.zeros(len(parties.index)))\n",
    "parties['parties'] = zero_len\n",
    "parties['parties'] = (parties.president == \"Truman\") | (parties.president == \"Kennedy\") | (parties.president == \"Johnson\") | (parties.president == \"Carter\") | (parties.president == \"Clinton\") | (parties.president == \"Obama\")\n",
    "parties.parties = list(map(lambda x: 1 if x else 0, parties.parties))\n",
    "parties = parties.iloc[:][parties.year >= 1945]\n",
    "# Set the parties variable as y\n",
    "y = parties.parties\n",
    "# Split the X and y variables into a training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start with the fun stuff. First, we first a logistic regression model and do some basic parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5045cc7c7315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Fit the model, predict and report the accuracy and the best parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the logistic regression estimator with an l1 loss parameter\n",
    "log_reg = LogisticRegression(penalty = \"l1\")\n",
    "# Set some parameters to tune over\n",
    "c_space = [1.3, 1.5, 1.7]\n",
    "parameters = {'C': c_space}\n",
    "# Create a cross-validation estimator\n",
    "cv = GridSearchCV(log_reg, parameters)\n",
    "# Fit the model, predict and report the accuracy and the best parameter\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll try with a logistic regression using the topics instead of the document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = count_words(prep_data)\n",
    "K, S, alpha, eta = 2, 1000, 0.1, 0.01\n",
    "Col_Gibbs = lda.LDA(n_topics=K, n_iter=S, alpha=alpha, eta=eta)\n",
    "X = Col_Gibbs.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 59)\n",
    "log_reg = LogisticRegression(\"l2\")\n",
    "c_space = [0.5, 2, 5]\n",
    "parameters = {'C': c_space}\n",
    "# Create a cross-validation estimator\n",
    "cv = GridSearchCV(log_reg, parameters)\n",
    "# Fit the model, predict and report the accuracy and the best parameter\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
