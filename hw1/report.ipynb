{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining for Economics and Finance: Homework 1\n",
    "\n",
    "## Roger Garriga Calleja, Javier Mas Adell, José Fernando Moreno Gutiérrez\n",
    "\n",
    "Data: Speech_data_extended.txt found on https://github.com/sekhansen/text-mining-tutorial.\n",
    "\n",
    "### Exercise 1: Pre-processing the data\n",
    "Packages used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with data from 1900 onwards to make the data more manageable. However, the same process could be used in the whole dataset. The data we are using corresponds to the speeches made by the different presidents of United States. Each row is a speech and it contains three columns: The president, the speech and the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the dataset.\n",
    "data = pd.read_table(\"speech_data_extend.txt\", encoding=\"utf-8\")\n",
    "data = data.loc[data['year']>=1900]\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pre-process the data in order to analyse it. There are four steps:\n",
    "- Tokenization (separate single words/tokens by spaces).\n",
    "- Removal of non-alphabetical characters.\n",
    "- Removal of stopwords (words that do not help diferenciate documents such as 'a', 'the', 'for', etc.).\n",
    "- Stemming (remove suffixes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "prep_data = data.apply(lambda row: nltk.word_tokenize(row['speech'].lower()), axis=1)\n",
    "\n",
    "#Removal of stop words and non-alphanumeric characters\n",
    "stop_w=set(stopwords.words('english'))\n",
    "for i in range(len(prep_data)):\n",
    "    prep_data[i] = [re.sub('[^\\\\w]','',w) for w in prep_data[i] if (re.sub('[^\\\\w]','',w) not in stop_w) and \n",
    "                    re.sub('[^\\\\w]','',w)!=''] \n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer() #Create a stemmer object\n",
    "for i in range(len(prep_data)):\n",
    "    prep_data[i] = [stemmer.stem(elem) for elem in prep_data[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create an tf-idf score for each word in the corpus and eliminate those that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
